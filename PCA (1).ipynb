{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nCuZncxPXIP"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "labels = iris.target_names\n",
        "\n",
        "# Step 2: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply PCA (keep all components to see full variance)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 4: Scree plot\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Reduce to 2D and visualize\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X_2d, columns=['PC1', 'PC2'])\n",
        "df['Species'] = [labels[i] for i in y]\n",
        "\n",
        "# Step 6: Scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for species in df['Species'].unique():\n",
        "    subset = df[df['Species'] == species]\n",
        "    plt.scatter(subset['PC1'], subset['PC2'], label=species)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA of Iris Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data                # Feature matrix\n",
        "y = iris.target              # Target labels (0, 1, 2)\n",
        "labels = iris.target_names   # Label names (setosa, versicolor, virginica)\n",
        "features = iris.feature_names\n",
        "\n",
        "# Step 2: Standardize the data (mean = 0, std = 1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply PCA with all components to understand variance distribution\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 4: Print explained variance ratio for each component\n",
        "print(\"Explained variance by each Principal Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {var:.2%}\")\n",
        "\n",
        "# Step 5: Scree plot (individual + cumulative variance)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o', label='Individual Variance')\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='s', linestyle='--', label='Cumulative Variance')\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.xticks(range(1, len(explained_variance)+1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Reduce to 2 principal components for 2D visualization\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_2d = pca_2d.fit_transform(X_scaled)\n",
        "\n",
        "# Create DataFrame for 2D plot\n",
        "df_2d = pd.DataFrame(X_2d, columns=['PC1', 'PC2'])\n",
        "df_2d['Species'] = [labels[i] for i in y]\n",
        "\n",
        "# Step 7: 2D Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for species in df_2d['Species'].unique():\n",
        "    subset = df_2d[df_2d['Species'] == species]\n",
        "    plt.scatter(subset['PC1'], subset['PC2'], label=species)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA of Iris Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 8: Reduce to 3 components for 3D visualization\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_3d = pca_3d.fit_transform(X_scaled)\n",
        "\n",
        "# Step 9: 3D Scatter Plot\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for idx, label in enumerate(labels):\n",
        "    ax.scatter(\n",
        "        X_3d[y == idx, 0],\n",
        "        X_3d[y == idx, 1],\n",
        "        X_3d[y == idx, 2],\n",
        "        label=label,\n",
        "        s=60\n",
        "    )\n",
        "\n",
        "ax.set_title(\"3D PCA of Iris Dataset\")\n",
        "ax.set_xlabel(\"PC1\")\n",
        "ax.set_ylabel(\"PC2\")\n",
        "ax.set_zlabel(\"PC3\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cCfo8NdcPZyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Detailed Explanation of the Code:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "Imports:\n",
        "numpy: For numerical operations like array manipulations.\n",
        "\n",
        "pandas: For data handling and manipulation.\n",
        "\n",
        "matplotlib.pyplot: For plotting graphs and visualizations.\n",
        "\n",
        "load_iris: From sklearn.datasets, used to load the famous Iris dataset.\n",
        "\n",
        "StandardScaler: From sklearn.preprocessing, used to standardize the dataset (mean = 0, standard deviation = 1).\n",
        "\n",
        "PCA: From sklearn.decomposition, used to perform Principal Component Analysis.\n",
        "\n",
        "Axes3D: For 3D plotting, which will be used later to visualize data in 3D.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data                # Feature matrix (input data)\n",
        "y = iris.target              # Target labels (output data)\n",
        "labels = iris.target_names   # Target names (species: setosa, versicolor, virginica)\n",
        "features = iris.feature_names # Feature names (sepal length, sepal width, petal length, petal width)\n",
        "Step 1: Loading Data:\n",
        "The Iris dataset is loaded using load_iris(), which provides the data in a structured form. This dataset consists of 4 features (sepal length, sepal width, petal length, petal width) and 150 samples of Iris flowers, divided into three species: Setosa, Versicolor, and Virginica.\n",
        "\n",
        "X contains the feature values (numerical data).\n",
        "\n",
        "y contains the labels (target values), which represent the species of each flower.\n",
        "\n",
        "labels stores the species names for clarity, and features stores the names of the attributes (sepal/petal dimensions).\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 2: Standardize the data (mean = 0, std = 1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Step 2: Standardizing the Data:\n",
        "PCA is sensitive to the scale of data. If the data is not standardized, features with larger ranges can dominate the principal components.\n",
        "\n",
        "To avoid this, we use StandardScaler to standardize the data by subtracting the mean and dividing by the standard deviation. This gives us a dataset where each feature has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "X_scaled is the standardized version of the dataset.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 3: Apply PCA with all components to understand variance distribution\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "Step 3: Apply PCA:\n",
        "We create an instance of the PCA class and fit it to the standardized data (X_scaled) using the fit_transform method. This performs PCA on the entire dataset.\n",
        "\n",
        "X_pca contains the transformed data in terms of principal components (PCs). By default, PCA will compute as many components as there are features (4 components in this case).\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 4: Print explained variance ratio for each component\n",
        "print(\"Explained variance by each Principal Component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {var:.2%}\")\n",
        "Step 4: Explained Variance:\n",
        "PCA identifies the directions (principal components) in which the data has the most variance. Each principal component explains a certain percentage of the total variance in the dataset.\n",
        "\n",
        "We print the explained variance ratio for each principal component. This tells us how much of the data's variance is captured by each component.\n",
        "\n",
        "pca.explained_variance_ratio_ provides the percentage of variance explained by each PC. We print this out to see how much each PC contributes to the overall data variation.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 5: Scree plot (individual + cumulative variance)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o', label='Individual Variance')\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='s', linestyle='--', label='Cumulative Variance')\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.xticks(range(1, len(explained_variance)+1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "Step 5: Scree Plot:\n",
        "The Scree plot is used to visualize the explained variance of each principal component.\n",
        "\n",
        "We plot both:\n",
        "\n",
        "Individual variance (variance explained by each component).\n",
        "\n",
        "Cumulative variance (total variance explained up to that component).\n",
        "\n",
        "The scree plot helps us understand how many components are required to capture a significant portion of the variance. You can identify the \"elbow\" point in the plot, which suggests the optimal number of components.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 6: Reduce to 2 principal components for 2D visualization\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_2d = pca_2d.fit_transform(X_scaled)\n",
        "Step 6: 2D PCA:\n",
        "To reduce the dimensionality of the dataset for easier visualization, we perform PCA with 2 components (n_components=2).\n",
        "\n",
        "X_2d is the transformed dataset with only the first two principal components. This allows us to plot the data in 2D.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Create DataFrame for 2D plot\n",
        "df_2d = pd.DataFrame(X_2d, columns=['PC1', 'PC2'])\n",
        "df_2d['Species'] = [labels[i] for i in y]\n",
        "Create DataFrame for 2D Visualization:\n",
        "We convert the reduced 2D data (X_2d) into a Pandas DataFrame for easy plotting.\n",
        "\n",
        "The Species column is added to label each data point according to its species (Setosa, Versicolor, or Virginica).\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 7: 2D Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for species in df_2d['Species'].unique():\n",
        "    subset = df_2d[df_2d['Species'] == species]\n",
        "    plt.scatter(subset['PC1'], subset['PC2'], label=species)\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA of Iris Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "Step 7: 2D Scatter Plot:\n",
        "A scatter plot is created to visualize the data in 2D using the first two principal components (PC1 and PC2).\n",
        "\n",
        "Each point is colored according to its species, making it easy to visually separate the Iris species based on their principal component values.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 8: Reduce to 3 components for 3D visualization\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_3d = pca_3d.fit_transform(X_scaled)\n",
        "Step 8: 3D PCA:\n",
        "To explore the dataset in 3D, we reduce the data to 3 components (n_components=3) and store the result in X_3d.\n",
        "\n",
        "This step helps visualize the data in 3D space.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Step 9: 3D Scatter Plot\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for idx, label in enumerate(labels):\n",
        "    ax.scatter(\n",
        "        X_3d[y == idx, 0],\n",
        "        X_3d[y == idx, 1],\n",
        "        X_3d[y == idx, 2],\n",
        "        label=label,\n",
        "        s=60\n",
        "    )\n",
        "\n",
        "ax.set_title(\"3D PCA of Iris Dataset\")\n",
        "ax.set_xlabel(\"PC1\")\n",
        "ax.set_ylabel(\"PC2\")\n",
        "ax.set_zlabel(\"PC3\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "Step 9: 3D Scatter Plot:\n",
        "A 3D scatter plot is created to visualize the data points in three dimensions using the first three principal components.\n",
        "\n",
        "The points are color-coded based on the species, making it easier to separate the classes visually in 3D space.\n",
        "\n",
        "The projection='3d' argument allows us to create a 3D plot."
      ],
      "metadata": {
        "id": "0HemtvdZPbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Concept of PCA:\n",
        "PCA (Principal Component Analysis) is a technique used for dimensionality reduction in datasets with multiple variables.\n",
        "\n",
        "It transforms the original data into a new set of orthogonal axes called Principal Components (PCs).\n",
        "\n",
        "The goal is to reduce the number of variables while retaining as much variance as possible from the data.\n",
        "\n",
        "PCA is widely used for:\n",
        "\n",
        "Data visualization (reducing dimensions to 2D or 3D).\n",
        "\n",
        "Noise reduction.\n",
        "\n",
        "Feature extraction.\n",
        "\n",
        "Steps in PCA:\n",
        "Standardize the Data:\n",
        "\n",
        "Standardize the data to ensure each feature has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑍\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "Z=\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑋\n",
        "X is the original data.\n",
        "\n",
        "𝜇\n",
        "μ is the mean of each feature.\n",
        "\n",
        "𝜎\n",
        "σ is the standard deviation of each feature.\n",
        "\n",
        "𝑍\n",
        "Z is the standardized data.\n",
        "\n",
        "Compute Covariance Matrix:\n",
        "\n",
        "The covariance matrix captures the relationship between different features in the data.\n",
        "\n",
        "Formula for covariance between two variables\n",
        "𝑥\n",
        "x and\n",
        "𝑦\n",
        "y:\n",
        "\n",
        "Cov\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "−\n",
        "1\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "Cov(x,y)=\n",
        "n−1\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " −\n",
        "x\n",
        "ˉ\n",
        " )(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "𝑛\n",
        "n is the number of samples.\n",
        "\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " ,y\n",
        "i\n",
        "​\n",
        "  are the individual data points.\n",
        "\n",
        "𝑥\n",
        "ˉ\n",
        ",\n",
        "𝑦\n",
        "ˉ\n",
        "x\n",
        "ˉ\n",
        " ,\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  are the mean values of\n",
        "𝑥\n",
        "x and\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "Compute Eigenvalues and Eigenvectors:\n",
        "\n",
        "Eigenvalues and eigenvectors are used to define the principal components.\n",
        "\n",
        "Eigenvectors give the direction of the new axes (principal components).\n",
        "\n",
        "Eigenvalues indicate the variance along the respective eigenvector.\n",
        "\n",
        "Sort Eigenvalues:\n",
        "\n",
        "The eigenvectors are sorted based on their eigenvalues in descending order.\n",
        "\n",
        "The largest eigenvalue corresponds to the principal component with the most variance.\n",
        "\n",
        "Select the Top k Principal Components:\n",
        "\n",
        "Choose the top k eigenvectors corresponding to the largest eigenvalues to form the new set of features (the reduced dataset).\n",
        "\n",
        "Transform the Data:\n",
        "\n",
        "The final step is to transform the original data into the new space defined by the selected eigenvectors (principal components).\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "new\n",
        "=\n",
        "𝑍\n",
        "×\n",
        "𝑉\n",
        "X\n",
        "new\n",
        "​\n",
        " =Z×V\n",
        "where:\n",
        "\n",
        "𝑍\n",
        "Z is the standardized data matrix.\n",
        "\n",
        "𝑉\n",
        "V is the matrix of eigenvectors (principal components).\n",
        "\n",
        "𝑋\n",
        "new\n",
        "X\n",
        "new\n",
        "​\n",
        "  is the transformed data in the reduced dimensional space.\n",
        "\n",
        "Key Terms:\n",
        "Variance: The amount of spread or dispersion in the dataset. PCA seeks to maximize variance along the new principal components.\n",
        "\n",
        "Covariance Matrix: A square matrix that shows the covariance between different features in the dataset.\n",
        "\n",
        "Eigenvalues: Measure the amount of variance captured by the corresponding eigenvector (principal component).\n",
        "\n",
        "Eigenvectors: Define the direction of the new axes (principal components) in the transformed space.\n",
        "\n",
        "Formulae:\n",
        "Covariance (between two features\n",
        "𝑥\n",
        "x and\n",
        "𝑦\n",
        "y):\n",
        "\n",
        "Cov\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "−\n",
        "1\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "Cov(x,y)=\n",
        "n−1\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " −\n",
        "x\n",
        "ˉ\n",
        " )(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "Standardization:\n",
        "\n",
        "𝑍\n",
        "=\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "Z=\n",
        "σ\n",
        "X−μ\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝜇\n",
        "μ is the mean.\n",
        "\n",
        "𝜎\n",
        "σ is the standard deviation.\n",
        "\n",
        "PCA Transformation:\n",
        "\n",
        "𝑋\n",
        "new\n",
        "=\n",
        "𝑍\n",
        "×\n",
        "𝑉\n",
        "X\n",
        "new\n",
        "​\n",
        " =Z×V\n",
        "where:\n",
        "\n",
        "𝑍\n",
        "Z is the standardized data.\n",
        "\n",
        "𝑉\n",
        "V is the matrix of eigenvectors (principal components).\n",
        "\n",
        "𝑋\n",
        "new\n",
        "X\n",
        "new\n",
        "​\n",
        "  is the transformed data.\n",
        "\n",
        "Eigenvalue Decomposition:\n",
        "\n",
        "𝐶\n",
        "⋅\n",
        "𝑉\n",
        "=\n",
        "𝜆\n",
        "⋅\n",
        "𝑉\n",
        "C⋅V=λ⋅V\n",
        "where:\n",
        "\n",
        "𝐶\n",
        "C is the covariance matrix.\n",
        "\n",
        "𝑉\n",
        "V is the eigenvector matrix.\n",
        "\n",
        "𝜆\n",
        "λ is the eigenvalue matrix.\n",
        "\n",
        "Explained Variance (per principal component):\n",
        "\n",
        "Explained Variance Ratio\n",
        "=\n",
        "𝜆\n",
        "𝑖\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜆\n",
        "𝑖\n",
        "Explained Variance Ratio=\n",
        "∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        " λ\n",
        "i\n",
        "​\n",
        "\n",
        "λ\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝜆\n",
        "𝑖\n",
        "λ\n",
        "i\n",
        "​\n",
        "  is the eigenvalue for the\n",
        "𝑖\n",
        "i-th principal component.\n",
        "\n",
        "Important Points to Remember:\n",
        "Standardization is essential to make sure that the features with larger ranges don't dominate the PCA transformation.\n",
        "\n",
        "The principal components (PCs) are orthogonal (uncorrelated) directions that maximize the variance in the data.\n",
        "\n",
        "The first principal component (PC1) captures the highest variance, PC2 captures the second-highest variance, and so on.\n",
        "\n",
        "Dimensionality reduction: By selecting only the top k PCs, you reduce the data's dimensionality while retaining most of the variance.\n",
        "\n",
        "The number of dimensions to reduce to (k) is often chosen by looking at the cumulative explained variance or the scree plot.\n",
        "\n",
        "Common Applications of PCA:\n",
        "Data visualization: Reducing the dataset to 2 or 3 dimensions for easier visualization.\n",
        "\n",
        "Noise reduction: By keeping only the most important principal components, you can reduce noise in the data.\n",
        "\n",
        "Feature extraction: Identifying the most important features (principal components) for downstream tasks like classification or clustering."
      ],
      "metadata": {
        "id": "QyP05iU9PiAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}